{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74749241-d43f-47cf-a107-df76f7487338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "#check the directory prixfixe for other files like utils, dataprocessor etc\n",
    "from prixfixe.autosome import AutosomeDataProcessor, AutosomeFirstLayersBlock, AutosomeCoreBlock, AutosomeFinalLayersBlock, AutosomeTrainer, AutosomePredictor\n",
    "from prixfixe.bhi import BHIFirstLayersBlock,BHICoreBlock\n",
    "from prixfixe.unlockdna import UnlockDNACoreBlock\n",
    "from prixfixe.prixfixe import PrixFixeNet\n",
    "# Load your new test dataframe (adjust the path as necessary)\n",
    "\n",
    "test_df = pd.read_csv('liba_fli1.csv') #mpra dataset for fli1 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e8b81-f9a4-44de-b44d-d80632d6c0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a30f54b-b215-47ce-9c6e-e7c2b3052984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    actagaggatagatctgtgggcatatgcgcgccgtggaccttcggc...\n",
      "1    actagcacgtacaaatatacaaccatcgaccggttaattatccttg...\n",
      "2    actcaagccgccacagacccgagggctcctagcacacacatagtaa...\n",
      "3    acttcctttacacaacgatctaacgagagcgctcacggcttatgtc...\n",
      "4    actgtggatattagactgaaaccagatttcgcgaaagcaaaccaag...\n",
      "Name: Seq, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove the first 12 characters from the 'Seq' column\n",
    "test_df['Seq'] = test_df['Seq'].str[12:]\n",
    "\n",
    "# Display the updated DataFrame to verify the change\n",
    "print(test_df['Seq'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2a0761-f44e-4115-ae9a-25cc7b4273ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     clusterID           CRS    TF Library  \\\n",
      "1068  State_7M  LibA.Seq1082  Fli1    LibA   \n",
      "1069  State_7M  LibA.Seq1083  Fli1    LibA   \n",
      "1070  State_7M  LibA.Seq1085  Fli1    LibA   \n",
      "1071  State_7M  LibA.Seq1095  Fli1    LibA   \n",
      "1072  State_7M  LibA.Seq1097  Fli1    LibA   \n",
      "\n",
      "                                                    Seq  nrepeats  \\\n",
      "1068  actagcacgtacaaatatacaaccatcgaccggttaattatccttg...         1   \n",
      "1069  actcaagccgccacagacccgagggctcctagcacacacatagtaa...         1   \n",
      "1070  actgtggatattagactgaaaccagatttcgcgaaagcaaaccaag...         1   \n",
      "1071  acttacttatcgctccgatgtgaaccaggtctgccctaccgcggtt...         1   \n",
      "1072  actatgctatccaacaggtgacttctagtcagtggggggttcgctc...         1   \n",
      "\n",
      "      affinitynum orientation  spacer  mean.norm.raw  mean.norm.adj  \\\n",
      "1068         1.00         fwd      10      -0.359004      -0.135383   \n",
      "1069         1.00         fwd      20       0.158773       0.382394   \n",
      "1070         1.00         rev      10      -0.281316      -0.057694   \n",
      "1071         0.75         fwd      20      -0.353558      -0.129937   \n",
      "1072         0.75         rev      10      -0.118013       0.105609   \n",
      "\n",
      "      mean.scaled.final  sum.biophys.affinity  \n",
      "1068          -0.060179             13.259571  \n",
      "1069           0.169978             13.259571  \n",
      "1070          -0.025646             13.259571  \n",
      "1071          -0.057758             10.250427  \n",
      "1072           0.046944             10.329291  \n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame with only the rows where 'clusterID' is 'State_7M'\n",
    "test_df = test_df[test_df['clusterID'] == 'State_7M'] #since the chip-seq data here was from CMP, I chose state 7M. Testing on the same sequences from other cell states would have been redundant.\n",
    "\n",
    "# Display the first few rows to verify the filtering\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19dfb8e-c9bb-4a2e-a027-48f8660a8c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/11767354/ipykernel_2458540/512105882.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "PrixFixeNet                                   [1, 1]                    --\n",
      "├─BHIFirstLayersBlock: 1-1                    --                        --\n",
      "│    └─ModuleList: 2-1                        --                        --\n",
      "│    │    └─ConvBlock: 3-1                    [1, 160, 250]             7,360\n",
      "│    │    └─ConvBlock: 3-2                    [1, 160, 250]             12,160\n",
      "├─AutosomeCoreBlock: 1-2                      --                        --\n",
      "│    └─ModuleDict: 2-2                        --                        --\n",
      "│    │    └─Sequential: 3-3                   [1, 320, 250]             420,048\n",
      "│    │    └─Sequential: 3-4                   [1, 128, 250]             573,696\n",
      "│    │    └─Sequential: 3-5                   [1, 128, 250]             173,856\n",
      "│    │    └─Sequential: 3-6                   [1, 128, 250]             229,632\n",
      "│    │    └─Sequential: 3-7                   [1, 128, 250]             87,072\n",
      "│    │    └─Sequential: 3-8                   [1, 64, 250]              114,816\n",
      "│    │    └─Sequential: 3-9                   [1, 64, 250]              45,968\n",
      "│    │    └─Sequential: 3-10                  [1, 64, 250]              57,472\n",
      "│    │    └─Sequential: 3-11                  [1, 64, 250]              45,968\n",
      "│    │    └─Sequential: 3-12                  [1, 64, 250]              57,472\n",
      "│    │    └─Sequential: 3-13                  [1, 64, 250]              45,968\n",
      "│    │    └─Sequential: 3-14                  [1, 64, 250]              57,472\n",
      "├─AutosomeFinalLayersBlock: 1-3               --                        --\n",
      "│    └─Conv1d: 2-3                            [1, 256, 250]             16,640\n",
      "│    └─Sequential: 2-4                        [1, 1]                    --\n",
      "│    │    └─Linear: 3-15                      [1, 1]                    257\n",
      "│    └─Conv1d: 2-5                            [1, 256, 250]             16,640\n",
      "│    └─Sequential: 2-6                        [1, 1]                    --\n",
      "│    │    └─Linear: 3-16                      [1, 1]                    257\n",
      "===============================================================================================\n",
      "Total params: 1,962,754\n",
      "Trainable params: 1,962,754\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 448.57\n",
      "===============================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 23.19\n",
      "Params size (MB): 7.85\n",
      "Estimated Total Size (MB): 31.04\n",
      "===============================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 8510.51it/s]\n",
      "100%|██████████| 105/105 [00:02<00:00, 47.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#this is everything that needs to be defined AND make the predictions. The training of the model is in training.ipynb ; since this is a new dataset, \n",
    "#some variables like model arhcitecture need to be defined again\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Model parameters that were saved in directory model_weight after training, sequence size and  cuda device id in order to use the gpu\n",
    "MODEL_LOG_DIR = \"model_weight\"\n",
    "#(change it depending on your data seq lnegth from chip and adjust mpra seq accordingly)\n",
    "SEQ_SIZE = 250\n",
    "CUDA_DEVICE_ID = 0\n",
    "\n",
    "# Set up device: use CUDA if available; otherwise, CPU. There werent too many sequences so i have used the cpu below. can be changed\n",
    "device = torch.device(f\"cuda:{CUDA_DEVICE_ID}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Import model building blocks from files in directory /prixfixe\n",
    "from prixfixe.autosome import AutosomeCoreBlock, AutosomeFinalLayersBlock\n",
    "from prixfixe.bhi import BHIFirstLayersBlock\n",
    "from prixfixe.prixfixe import PrixFixeNet\n",
    "\n",
    "# Build the model architecture, this is defined also in training.ipynb and needs to be the same. You may use 3 architectures as is or create your own. \n",
    "#There is an ipynb here explaining how you can do that https://github.com/de-Boer-Lab/random-promoter-dream-challenge-2022/blob/main/Test_Your_NN_In_Prix_Fixe.ipynb\n",
    "\n",
    "first = BHIFirstLayersBlock(\n",
    "    in_channels=5,\n",
    "    out_channels=320,\n",
    "    seqsize=SEQ_SIZE,\n",
    "    kernel_sizes=[9, 15],\n",
    "    pool_size=1,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "core = AutosomeCoreBlock(\n",
    "    in_channels=first.out_channels,\n",
    "    out_channels=64,\n",
    "    seqsize=first.infer_outseqsize()\n",
    ")\n",
    "\n",
    "final = AutosomeFinalLayersBlock(in_channels=core.out_channels)\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(42)\n",
    "\n",
    "model = PrixFixeNet(\n",
    "    first=first,\n",
    "    core=core,\n",
    "    final=final,\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "from torchinfo import summary\n",
    "print(summary(model, (1, 5, SEQ_SIZE)))\n",
    "\n",
    "# Load the pre-trained model weights with map_location to ensure compatibility with CPU if needed\n",
    "model_weights_path = f\"{MODEL_LOG_DIR}/model_best_MSE.pth\"\n",
    "model.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Add a 'rev' column if it doesn't exist (assume all sequences are forward)\n",
    "if 'rev' not in test_df.columns:\n",
    "    test_df['rev'] = 0\n",
    "\n",
    "# One-hot encoding function for nucleotides\n",
    "def one_hot_encode(seq):\n",
    "    mapping = {\n",
    "        'A': [1, 0, 0, 0],\n",
    "        'G': [0, 1, 0, 0],\n",
    "        'C': [0, 0, 1, 0],\n",
    "        'T': [0, 0, 0, 1],\n",
    "        'a': [1, 0, 0, 0],\n",
    "        'g': [0, 1, 0, 0],\n",
    "        'c': [0, 0, 1, 0],\n",
    "        't': [0, 0, 0, 1],\n",
    "        'N': [0, 0, 0, 0],\n",
    "        'n': [0, 0, 0, 0]\n",
    "    }\n",
    "    return [mapping[base] for base in seq]\n",
    "\n",
    "# One-hot encode sequences from the 'seq' column containing the sequence which should now be 250 and append the 'rev' value\n",
    "encoded_seqs = []\n",
    "Y_test_dev = []\n",
    "Y_test_hk = []\n",
    "\n",
    "for i, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    seq_str = row['Seq']\n",
    "    encoded_seq = one_hot_encode(seq_str)\n",
    "    # Append the 'rev' value to each one-hot encoded base\n",
    "    rev_value = [row['rev']] * len(encoded_seq)\n",
    "    encoded_seq_with_rev = [list(encoded_base) + [rev] for encoded_base, rev in zip(encoded_seq, rev_value)]\n",
    "    encoded_seqs.append(encoded_seq_with_rev)\n",
    "   \n",
    "    \n",
    "    # Optionally, collect true labels if available\n",
    "    if 'Dev_log2_enrichment' in test_df.columns:\n",
    "        Y_test_dev.append(row['Dev_log2_enrichment'])\n",
    "    if 'Hk_log2_enrichment' in test_df.columns:\n",
    "        Y_test_hk.append(row['Hk_log2_enrichment'])\n",
    "\n",
    "# Make predictions for each sequence\n",
    "pred_expr_dev = []\n",
    "pred_expr_hk = []\n",
    "\n",
    "for seq in tqdm(encoded_seqs):\n",
    "    # Reshape to (1, SEQ_SIZE, 5) and then transpose to (1, 5, SEQ_SIZE)\n",
    "    seq_array = np.array(seq).reshape(1, SEQ_SIZE, 5).transpose(0, 2, 1)\n",
    "    seq_tensor = torch.tensor(seq_array, device=device, dtype=torch.float32)\n",
    "    pred = model(seq_tensor)\n",
    "    # Assuming pred[0] is Dev enrichment and pred[1] is Hk enrichment\n",
    "    pred_expr_dev.append(pred[0].detach().cpu().flatten().tolist())\n",
    "    pred_expr_hk.append(pred[1].detach().cpu().flatten().tolist())\n",
    "\n",
    "# Optionally, add predictions to your dataframe and save to file\n",
    "test_df['pred_dev'] = pred_expr_dev\n",
    "test_df['pred_hk'] = pred_expr_hk\n",
    "#test_df.to_csv(\"test_predictions.tsv\", sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac58b63-66a3-4758-bde5-1a21cb742510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
